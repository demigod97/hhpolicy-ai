# Story 1.4: Basic RAG Ingestion for Administrator Policies - Brownfield Addition

## Status
Ready

## Story
**As an** Administrator,
**I want** my uploaded document to be automatically processed and indexed,
**So that** its content becomes searchable.

## Story Context

**Existing System Integration:**
- Integrates with: Story 1.3 document upload pipeline, existing N8N workflows, Supabase vector store
- Technology: N8N workflow automation, document text extraction, vector embeddings, Supabase vector storage
- Follows pattern: Existing N8N document processing patterns from InsightsLM
- Touch points: File upload → N8N trigger → Text extraction → Chunking → Vector storage → Status update

## Acceptance Criteria

**Functional Requirements:**
1. A successful document upload triggers the N8N ingestion workflow automatically
2. The workflow extracts the text from the document (PDF, TXT formats)
3. The extracted text is chunked and upserted into the Supabase vector store, along with metadata linking it to the correct `policy_document_id`
4. Upon completion, the document's processing status is updated to 'completed'

**Integration Requirements:**
5. Existing document upload functionality (Story 1.3) continues to work unchanged
6. New RAG ingestion follows existing N8N workflow patterns
7. Integration with Supabase vector store maintains current schema and performance

**Quality Requirements:**
8. Text extraction handles both PDF and TXT formats reliably
9. Vector chunking produces optimal chunk sizes for retrieval accuracy
10. Processing completes within reasonable time limits (under 5 minutes for typical documents)

## Technical Notes

- **Integration Approach:** Extend existing N8N document processing workflows to handle policy documents with proper metadata linking
- **Existing Pattern Reference:** Leverage existing text extraction and vector storage patterns from InsightsLM
- **Key Constraints:** Must maintain document-to-chunks traceability for citation purposes

## Definition of Done

- [ ] Functional requirements met
- [ ] Integration requirements verified
- [ ] Existing functionality regression tested
- [ ] Code follows existing patterns and standards
- [ ] Tests pass (existing and new)
- [ ] Documentation updated if applicable

## Risk and Compatibility Check

**Minimal Risk Assessment:**
- **Primary Risk:** Document processing failures could leave documents in 'processing' state permanently
- **Mitigation:** Implement robust error handling and retry mechanisms in N8N workflow
- **Rollback:** Disable automatic processing, allow manual processing trigger

**Compatibility Verification:**
- [ ] No breaking changes to existing APIs
- [ ] Database changes leverage existing policy_documents schema
- [ ] Vector storage follows existing patterns
- [ ] Performance impact is acceptable

## Tasks / Subtasks

- [ ] **Task 1: N8N Workflow Enhancement**
  - [ ] Create/update N8N workflow for policy document ingestion
  - [ ] Implement PDF text extraction with error handling
  - [ ] Add TXT file processing support
  - [ ] Ensure proper workflow triggering from upload completion

- [ ] **Task 2: Text Chunking & Vector Storage**
  - [ ] Implement optimal text chunking strategy for policy documents
  - [ ] Generate vector embeddings using existing LLM integration
  - [ ] Store chunks in Supabase vector store with policy_document_id linking
  - [ ] Maintain chunk-to-document traceability for citations

- [ ] **Task 3: Processing Status Management**
  - [ ] Update document processing status upon workflow completion
  - [ ] Handle processing failures with appropriate error states
  - [ ] Implement processing progress tracking if needed
  - [ ] Add logging for debugging and monitoring

- [ ] **Task 4: Quality & Performance Testing**
  - [ ] Test document processing across various file sizes
  - [ ] Validate text extraction accuracy for different PDF formats
  - [ ] Verify vector storage and retrieval functionality
  - [ ] Performance test with realistic document volumes

## Dev Notes

### N8N Workflow Architecture
- Trigger: Document upload completion from Story 1.3
- Steps: File retrieval → Text extraction → Chunking → Embedding → Vector storage → Status update
- Error handling: Retry logic, failure notifications, status updates

### Text Processing Pipeline
1. **File Retrieval**: Download from Supabase Storage using secure URL
2. **Text Extraction**: PDF parsing (pdf-parse) or direct text reading
3. **Chunking**: Semantic chunking with overlap for better retrieval
4. **Embedding**: Generate vectors using OpenAI/Gemini embeddings
5. **Storage**: Upsert to Supabase vector store with metadata

### Metadata Structure
- policy_document_id: Links chunks back to source document
- chunk_index: Position within document
- chunk_text: Extracted text content
- embedding: Vector representation
- document_metadata: Title, role_assignment, upload date

### Key Files to Create/Modify
- N8N workflow files for policy document ingestion
- Vector storage schema updates if needed
- Processing status update mechanisms
- Error handling and retry logic

### Testing Strategy
- **Unit Tests**: Text extraction, chunking logic
- **Integration Tests**: Complete N8N workflow execution
- **Performance Tests**: Processing time and resource usage
- **Error Tests**: Various failure scenarios and recovery

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-01-19 | 1.0 | Initial story creation for RAG ingestion (PRD Story 1.4) | Sarah (PO) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be added here*